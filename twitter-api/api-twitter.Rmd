---
title: "Mendapatkan data dari Twitter menggunakan API"
author: "Ujang Fahmi"
date: "4/5/2018"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Pendahuluan
Di sini saya akan belajar bagaimana caranya mendapatkan data dari Twitter menggunakan API gratis. Berikut adalah beberapa langkahnya. Tujuan utama tuturial ini adalah mendapatkan data dulu, proses kemudian. 

1. Membuat API di twitter
2. Setting API buat ambil data di R
3. Running di R buat ambil data
4. Menyimpan data yang didapat 
5. Mengolah data yang didapat

# Membuat API Twitter
Twiter menyediakan tiga macam Application programming interface (API), yaitu gratis, bisnis, dan premium. Perbedaan ketiganya adalah jumlah data yang bisa didapat dan lamanya waktu twit yang bisa didapat. Kali ini kita akan mengambil data dengan API Gratis. Langkahnya ada sebagai berikut. 

1. Go to https://apps.twitter.com/app/new and log in, if necessary.
2. Enter your desired Application Name, Description and your website address making sure to enter the full address including the http://. You can leave the callback URL empty. ![Caption for the picture.](/Volumes/mydata/RStudio/belajaR/twitter-api/img1.png)
3. Accept the TOS and submit the form by clicking the Create your Twitter Application.
4. After creating your Twitter Application click on the tab that says Keys and Access Tokens, then you have to give access to your Twitter Account to use this Application. To do this, click the Create my Access Token button. ![Caption for the picture.](/Volumes/mydata/RStudio/belajaR/twitter-api/img2.png)
5. Lastly copy the Consumer key (API key), Consumer Secret, Access Token and Access Token Secret from the screen into our pluginâ€™s Twitter Options page and test. ![Caption for the picture.](/Volumes/mydata/RStudio/belajaR/twitter-api/img3.png)

# Setting API buat ambil data di R

Setelah membuat aplikasi API Twitter selanjutnya kita bisa mengambil key dari aplikasi kita dan digunakan di R untuk mengambil data dari Twitter. Di R terdapat beberapa `library` yang dapat digunakan untuk mendapatkan data dari Twitter. Untuk itu kita akan terlebih dahulu menginstall dua di antaranya, yaitu `SocialMediaLab` dan `twitteR`. Untuk meninstallnya cukup menggunakan kode berikut: `install.packages("twitteR")` dan `install.packages("SocialMediaLab")`. Untuk bisa menjalankan kode tersebut, kita terlebih dahulu harus menginstall `devtools` dengan `install.packages("devtools")`. Setelah dua library atau package tersebut terinstall lalu kita panggil untuk digunakan dalam script. 

## Library
```{r lib, echo=TRUE, results='hide', message=FALSE}
library(SocialMediaLab)
library(twitteR)
# library tambahan
library(dplyr)
library(igraph)
library(skimr)
```


```{r sett1, echo=TRUE, results='hide'}
# REPLACE WITH YOUR API KEY
myapikey <- "pAFA3zX08uixfVtP4PMpcHas0"
# REPLACE WITH YOUR API SECRET
myapisecret <- "FZfuzn055vuJgRFraq8KNAAWipsa0ugUEpIjWxuhOFH9Kd5HAm"
# REPLACE WITH YOUR ACCESS TOKEN
myaccesstoken <- "73705532-JApWQavtY5kkKbpRUGpDByEhHdLxb2HcKzAgtDZ7T"
# REPLACE WITH YOUR ACCESS TOKEN SECRET
myaccesstokensecret <- "2p2xuhlsMHVlbqDx8Swb7IkCAstwmCEMoINYreNmWxoCN"
```

Script di atas digunakan untuk memanggil API milik kita sudah dibuat. Selanjutnya, script di bawah ini digunakan untuk mendapatkan data dengan kata kunci tertentu. Kata kunci ini bisa berupa hashtag, kata, gabungan kata, atau username Twitter. Sebagai contoh di sini saya akan menggunakan kombinasi kata **uu-md3 DPR**. 

# Running di R buat ambil data
Di sini kita akan mencoba dua librar, yaitu socialMediaLab dan twitteR, dan melihat perbedaan hasilnya. 

## SocialMediaLab
Package ini dapat digunakan untuk mengambil data dari Twitter, dan juga langsung *mengenerate* input untuk network analysis yang bisa dilakukan di software lain. Misalnya **Gephi**. Untuk menggunakannya, kita terlebih dahulu harus mengatur pengunaan API Twitter yang sudah dibuat. Keterangan lebih lengkan dan lanjut tentang socialMediaLab dapat ditemukan [di sini](https://bit.ly/2GYs2JG). 

```{r sett2, echo=TRUE, results='hide', message=FALSE}
data1 <- Authenticate("twitter", apiKey=myapikey,
                             apiSecret=myapisecret,
                             accessToken=myaccesstoken,
                             accessTokenSecret=myaccesstokensecret) %>%
  Collect(searchTerm="uu-md3 DPR", numTweets=10000,
          writeToFile=FALSE,verbose=TRUE)
```

Dengan script di atas, kita bisa mendapatkan tweet yang diposting selama 7 haris terakhir sebanyak 4192. Namun jika dilihat, tidak semua datanya memiliki text atau konten twitnya tidak terambil. Hal ini dikarenakan orang yang memposting twit tersebut tidak membuka akunnya, alias digembok. Inilah salah satu perbedaan socialMediaLab, dengan Twitter. `socialMediaLab` cenderung lebih fokus ke aktor atau usernamenya di banding kekontennya. Berikut adalah cara untuk melihat rangkuman data yang didapat dengan menggunakan library `skimr`. Seperti biasa, jika belum terinstaall bisa menginstallnya terlebih dahulu dengan: `install.packages("skimr")`.

```{r rangk1, echo=TRUE}
data1$text <- iconv(data1$text, to = 'utf-8-mac')
skim(data1)
```

Dari 4192 tweet yang didapat, yang memiliki konten hanya 219, hal ini dapat dilihat pada kolom `text`. Untuk memastikannya, kita juga bisa menggunakan subsetting data berdasarkan kolom `text` yang terisi. 

```{r subset1, echo=TRUE, results='hide'}
tweet_text <- data1 %>%
  filter(!text == "NA")
```

Hasilnya dapat dilihat di data baru `tweet_text`, di mana kita tetap masih memiliki 21 variabel atau kolom, namun observasi atau row nya hanya 219. Kita tinggalkan konten, di mana bukan menjadi salah satu fokus dari library ini. Mari kita lihat networknya penggunanya. 

### User network

```{r net1, echo=TRUE, results='hide'}
# get the actor
data1_net <- data1 %>% 
  Create("Actor") 
# We can now examine the description of our network
data1_net
```

Selanjutnya, kita bisa menyimpan file network yang sudah dibuat di sini dan di lihat di **gephi**. Untuk melakukannya kita terlebih dahulu harus memanggil library `igraph`. 

```{r net2, echo=TRUE, results='hide'}
# Saving graph data with graphml format for gephi
write.graph(data1_net, "uu-md3-net.graphml", format="graphml") 
```

Dengan data yang didapat menggunakan library `socialMediaLab` ini, mungkin pertanyaan yang relevan bukan tentang kontennya. Melainkan tentang distribusi dan aktor yang terlibat didalamnya. Misalnya, siapa sih 10 aktor utamanya yang berada dalam twit ini?

```{r aktor1, echo=TRUE}
pageRank_actor <- sort(page.rank(data1_net)$vector,decreasing=TRUE)
head(pageRank_actor,n=10)
```

Data di atas menunjukkan nama akun yang terlibat dalam twit yang mengandung kata uu-md3 dpr yang kita cari dengan library `socialMediaLab`. Pertanyaan lainnya, siapa yang paling tidak penting (dengan segala hormat) dalam jaringan ini?

```{r}
tail(pageRank_actor,n=10)
```

Pertanyaan-pertanyaa berikut ini saya copy dari laman socialMediaLab:
Is there any kind of community structure within the user network?

```{r, echo=TRUE, message=FALSE}
imc <- infomap.community(data1_net, nb.trials = 20)
```

Script di atas menghitung modularity dari network yang ada. 

```{r, echo=TRUE, message=FALSE, results='hide'}
# create a vector of users with their assigned community number
communityMembership <- membership(imc)
# summarise the distribution of users to communities
commDistribution <- summary(as.factor(communityMembership))
# which community has the max number of users
tail(sort(commDistribution),n=1)
# create a list of communities that includes the users assigned to each community
communities_md3 <- communities(imc)
# look at the members of the most populated community
communities_md3[names(tail(sort(commDistribution),n=1))]
# same as doing it manually
```

Siapa saja yan berada di komunitas 2
```{r, echo=TRUE}
communities_md3[2]
```
### Semantic Network
Dalam keterangannya, library `socialMediaLab` juga menyediakan fungsi untuk membuat semantic network, namun di sini saya tidak akan mencobanya secara langsung, karena umumnya text dari Twitter harus di Cleaning terlebih dahulu. 

## twitteR
Informasi tentang library ini bisa ditemukan [di sini](https://bit.ly/1TZNU2F). Pada dasarnya script dibawah ini adalah sama dengan yang diatas, di sini saya tulis kembali hanya untuk memudahkan saja. 

```{r, echo=TRUE, message=FALSE, results='hide'}
options(httr_oauth_cache=T)

api_key <- "pAFA3zX08uixfVtP4PMpcHas0"
api_secret <- "FZfuzn055vuJgRFraq8KNAAWipsa0ugUEpIjWxuhOFH9Kd5HAm"
token <- "73705532-JApWQavtY5kkKbpRUGpDByEhHdLxb2HcKzAgtDZ7T"
token_secret <- "2p2xuhlsMHVlbqDx8Swb7IkCAstwmCEMoINYreNmWxoCN"

setup_twitter_oauth(api_key, api_secret, token, token_secret)
```

Untuk mendapatkan data dengan library `twitteR` kita bisa menggunakan fungsi `searchTwitter()` dari library ini. Selain fungsi tersebut juga masih ada beberapa fungsi lain, misalnya untuk mengetahui trending, user timeline, dan lain sebagainya. Dengan menggunakan kata kunci yang sama, di sini saya mencoba mendapatkan 10000 twit, jika ada dalam seminggu terakhir. Parameter bahasa digunakan untuk membatasi. Untuk menjadikannya data frame menggunakan fungsi `twListToDF()`

```{r, echo=TRUE, message=FALSE, results='hide'}
# Run Twitter Search
data2 <- searchTwitter("uu-md3 DPR", n=10000, lang="id")
# Transform tweets list into a data frame
data2 <- twListToDF(data2)
```
Ternyata hasilnya tidak jauh berbeda dengan peribaan pertama, di mana di sini kita juga hanya bisa mendapatkan 4197 observasi atau twit. Bedanya, di sini kita bisa mendapatkan data twit yang lengkap, sejumlah observasi yang didapat. 

# Menyimpan data yang didapat
Sampai di sini kita sudah bisa mendapatkan dua data, yaitu `data1` dan `data2` yang dapat diolah lebih lanjut. Sampai di sini pula, tujuan kita sudah tercapat, yaitu mendapatkan data dengan cepat dari Twitter dengan menggunakan API yang gratis. Untuk pengolahan lebih lanjut, kita akan simpan data tersebut dengan script berikut. Berdasarkan pengalaman, kita akan simpan dalam format `.tsv` agar struktur data lebih terjaga. Karena di data1 yang dihasilkan oleh library `socialMediaLab` terdapat kolom yang berisi list, kita harus terlebih dahulu mengonversinya ke dalam format standar data frame untuk ditulis. 

```{r, echo=TRUE, message=FALSE}
library(tidyverse)
# konversi
data1 <- as.data.frame(data1)
data1 <- data.frame(lapply(data1, as.character), stringsAsFactors=FALSE)
# simpan
write_tsv(data1, "data1.tsv")
write_tsv(data2, "data2.tsv")
```

# Ringkasan
Bredasarkan percobaan di atas, diketahui bahwa dua library yang digunakan dapat menghasilkan data dengan jumlah yang identik. Pemilihan diantara keduanya harus didasarkan pada tujuan pengambilan data. Untuk mengetahui aktor degan cepat akan lebih baik menggunakan `socialMediaLab`. Namun jika tujuannya adalah untuk mengetahui konten, maka `twitteR` adalah pilihan yang harus dipertimbankan pertama untuk pengambilan data menggunakan API. 