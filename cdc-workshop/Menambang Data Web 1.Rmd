---
title: "Web Scrapping Menggunakan R"
author: "Ujang fahmi"
date: "7/9/2018"
output:
  html_notebook:
    after_body: footer.Rhtml
    includes: null
    number_sections: yes
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
  word_document:
    toc: yes
    toc_depth: '3'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

>**Keterangan:** Materi ini terdiri dari 1751 term/kata yang kurang lebih dapat dibaca sampai dengan selesai dalam waktu 8-10 menit.

# Pengantar
Web scraping juga disebut screen scrapping karena pada intinya kita memang hanya men-scrapp-apa yang sudah terbuka di layar (monitor). Web scrapping juga disbeut web data extraction karena hal tersebut kita lakukan memang untuk mendapatkan data. Web scrapping juga di sebut harvesting atau panen tanpa harus menanam karena data dibuat bukan oleh kita sendiri, tapi ditinggalkan sebagai jejak digital orang yang menggunakan internet. Secara umum scraping disebut teknik untuk mengekstrak data dalam jumlah bersar dari website, di mana data yang diekstrak kemudian di simpan dalam memory lokal, biasanya dalam format database dalam bentuk spreedsheet.  

Sederhananya, dengan melakukan scraping kita mengubah web menjadi data sehingga bisa digunakan untuk keperluan lebih lanjut. Proses mengubah web menjadi data melibatkan banyak hal (tidak mau bilang kompleks) yang harus dilakukan secara sistematis. Beruntungnya beberapa laman memiliki fasilitas API yang dapat digunakan dengan mudah dan nyaman. Sayangnya tidak semua API diberikan secara gratis. Dalam konteks Twitter, perusahaan tersebut memiliki empat jenis layanan API, yaitu *basic, standard, premium, dan enterprise*. Selain yang basic, semua layanan API harus ditebus. Informasi tentang API Twitter dapat ditemukan di [laman ini](https://developer.twitter.com/en.html).  

![Gambar 1. Mengubah web menjadi data](/Volumes/mydata/RStudio/belajaR/cdc-workshop/scraper1.png)

Jika karena satu atau lebih alasan kita tidak dapat menggunakan API, maka untuk mendapatkannya data mentah harus diolah dengan menggunakan teknik scraping atau semi automatic scraping (SAM). 

Secara teknis, SAM dengan API memiliki prinsip yang sama. Hanya dengan menggunakan API kita **tidak perlu** mempelajari struktur web (misal dengan html/java script). Sementara dengan menggunakan SAM, kita perlu melakukan eksperimen dengan struktur web yang akan di ambil datanya. 
Dengan mempelajari kode-kode konsisten yang disusun untuk dapat menampilkan gambar di sebalah kanan, kita bisa mendapatkan data secara otomatis. Proses pada umumnya di awali dengan mempelajari karakter web, mengunduh file html dan kemudian mengekstraknya.

Pada kesempatan ini, kita akan mencoba kedua hal tersebut dengan menggunakan bahasa pemrograman R. Untuk itu, kita perlu melakukan persiapan terlebih dahulu. 

# Persiapan
## Menyiapakan kebutuhan 
Untuk bisa melakukan scrapping kita membutuhkan: 

1. Memiliki akun Twitter (jika mau menscrap Twitter)
2. Mengunduh dan menginstall R, yang bisa didapat [**di sini**](https://repo.bppt.go.id/cran/).
3. Mengunduh dan menginstall R Studio, yang bisa didapat [**di sini**](https://www.rstudio.com/products/rstudio/download/#download).
4. Menggunakan google chrome

**Catatan:** Perbedaan program dengan bahasa pemrograman adalah kita bisa melakukan hal yang lebih fleksibel dengan bahasa. Dengan program kita hanya terbatas pada kemampuan/fitur yang sudah di tetapkan pembuatnya. Kelemahannya, memahami bahasa membutuhkan lebih banyak usaha dibanding memhami/menggunakan program. 

## Memahami alat (R dan RStudio)
Sama seperti bahasa yang digunakan oleh manusia untuk berkomunikasi, bahasa pemrograman juga memiliki aturan. Di sini saya ingin mengenalkan dua hal. Pertama tentang di tempat di mana bahasa tersebut digunakan. Kedua, tentang beberapa prinsip dasar yang wajib diketahui. 

### Antarmuka RStudio 
R adalah bahasanya, RStudio adalah salah satu alat untuk menggunakan bahasa tersebut. Berikut adalah tampilan RStudio, alat yang akan lebih banyak digunakan dalam latihan. 

![Gambar 2. Interface Rstudio](/Volumes/mydata/RStudio/belajaR/cdc-workshop/image35.png)

**Keterangan:** Masing-masing gambar (1-4) menunjukkan hal-hal berikut.

1. Menunjukkan tempat di mana kita akan menulis script (.R, .Rmd, dan atau lainnya)
2. Menunjukkan proses yang sedang berjalan setelah skrip di jalankan.
3. Menunjukkan tempat di mana kita bisa melihat data yang digunakan/bisa di analisis (bisa dibuat di dalam/bisa diekspor dari luar/memory)
4. Menunjukkan tempat di mana kita bisa melihat visualisasi skrip/analisis/pengolahan data secara langsung setelah skrip dijalankan. 

### Prinsip dasar

**1. Membuat Project**

Project dibuat untuk memastikan pengolahan atau hal yang dilakukan dengan menggunakan RStudio berada dalam satu directory/folder yang sama. Hal ini dibutuhkan untuk memudahkan penguunaan skrip. Cara membuatnya adalah sebagai berikut. 

> File -> New Project -> New Directory -> Directry Name -> Browse Folder/atau buat baru. 

Setelah projek dibuat, data dan skrip yang akan diolah (lebih baik) ditempatkan dalam folder yang sama

**2. Membuat dan Menulis skrip**

Rstudi dapat digunakan untuk membuat beberapa jenis skrip dengan tujuan dan fungsi berbeda. Selain bahasa R, Rstudio juga dapat digunakan untuk menulis bahasa pemrograman lain, misalnya **python** atau **C**. Dua skrip yang paling umum digunakan ada .R (dot R) dan .Rmd (dot Rmd atau Rmarkdown). Dot Rmd sangat nyaman digunakan untuk membuat laporan secara langsung. Kekurangannya, dot Rmd membutuhkan lebih banyak memory. 

> Membuat skrip dot R: File -> New File -> Rscript atau CMD + Shift + N (Mac)

> Membuat skrip dot Rmd: File -> New File -> R Markdown -> Membuat judul -> Memilih output file yang diinginkan. 

Setelah file skrip dibuat bisa langsung di simpan dengan nama yang sesuai. File skrip akan tersimpan secara otomatis dalam folder project yang dibuat. 

Seperti bahasa pemrograman lainnya, R juga bisa menjadi kalkulator. Berikut adalah contohnya. 
```{r}
2 + 3
```

Penjumlahan di atas bisa kita masukan menjadi variabel dengan cara menunjukkanya dengan tanda **=** atau **->**. Contohnya adalah:
```{r}
a = 2 + 3
a
```
Seperti telah dijelaskan sebelumnya di kolom 3 saat ini seharusnya ditemukan ada data a yang berisi angka 5. a dalam R disebut sebagai variabel/data.

Pada saat menulis skrip, kita mungkin akan kesusahan untuk mengingat semua tahapan yang kita tulis. Untuk membantu mengetahui fungsi skrip kita bisa membuat komentar. Dalam R komentar di tandai dan di awali dengan tanda **#** (tanda pagar). Contohnya:
```{r}
a = 2 + 3 # menambahkan .... 
a # menampilkan hasil
```
Dapat dilihat bahwa keterangan tidak mengubah hasil, karena keterangan tidak dibaca oleh komputer sebagai bahasa. Keterangan di sini berfungsi untuk manusianya. 

Fungsi dasar yang juga penting untuk diketahui adalah fungsi `summary(...)`. Fungsi ini digunakan untuk merangkum data dan sangat membantu dalam eksplorasi awal. Berikut adalah contohnya. 

```{r}
summary(mtcars)
```

Fungsi berikutnya adalah `str()` yang dapat digunakan untuk mengetahui struktur data yang akan dianalisis.
```{r}
str(mtcars)
```
dengan menjalankan fungsi di atas, kita tahu bahwa tiap kolom berisi variabel numeric. Namun dalam kasus lain mungkin kita menemukan variabel yang dimaksud tidak dalam format yang seharusnya. Misalnya tanggal 21-10-2108 dalam formta text atau chr (character) maka dengan fungsi `str()` kita tahu harus mengubah format data menjadi format yang standar di R.

**3. Menginstall Packages**

Di dalam R kita bisa memanfaatkan banyak packages yang dibuat oleh orang lain. Untuk menggunakannya kita harus tahu terlebih dahulu apa yang ingin dilakukan atau dibutuhkan. Dalam konteks ini, untuk scrapping ada beberapa packages yang bisa di manafaatkan dan akan dijelaskan pada bagian selanjutnya. Untuk dapat menggunakan package kita perlu: (1) menginstallnya dalam Rstudio; (2) memanggilnya dalam Rstudio. Caranya adalah: 

> Menginstall cara 1: ketik **install.packages("nama packages")** misalnya **install.packages("tidyverse")**

> Menginstall cara 2: lihat bagian atas dan klik **Tools** -> **install packages** -> **ketik nama packages** -> **install**

Catatan: (1) Untuk menginstall dengan mudah kita harus terkoneksi dengan internet. (2) Packages adalah kumpulan fungsi yang jika kita ingin menggunakannya perlu kita panggil terlebih dahulu.

> Memanggil package: library(nama package), ex: library(tidyverse)

Fungsi `library(...)` harus selalui dijalankan terlebih dahulu sebelum menggunakan fungsi-fungsi yang ada di dalam packages yang akan digunakan. 

**4. Impor dan Ekspor Data**

Untuk dapat di olah, data perlu di impor. Sebaliknya, hasil olahan juga bisa diekspor. R cukup fleksibel untuk mengolah hampir semua jenis data, mulai dari yang dihaslikan oleh software lain, misalnya stata (.rda) hingga data sederhana dalam bentuk, misalnya .csv. Secara prinsip, data harus dibaca oleh program, untuk itu fungsinya pun **read. ...** biasanya diikuti oleh jenis datanya. Berikut adalah contoh skrip untuk mengimpor data dalam format csv. Di sini saya menggunakan fungsi **read.csv** yang merupakan fungsi dasar (bawaan) sehingga tidak perlu memanggil **packages** apapun. 

**Impor data**
```{r}
data1 <- read.csv("raw_jkw.csv", stringsAsFactors = FALSE, header = TRUE, sep = ",")
```

Dengan menggunakan skrip di atas kita baru saja membuat sebuah data baru yang bernama **data1** (dapat dilihat pada environment/no.3). Hal yang perlu diperhatikan dalam membaca file dalam format .csv adalah **header** dan **separator**. 

Header merujuk pada apakah data memiliki header atau tidak. Sementara separator (sep) merujuk pada pembatas antar variable. Dalam data yang diimpor di atas data memiliki header, dan batas antar variabelnya berupa tanda koma. 

**ekspor data**

Jika untuk mengimpor fungsinya dimulai dengan **read**, maka untuk mengekspor dimulai dengan **write. ...**

```{r}
write.csv(data1, "tes ekspor data.csv")
```

Setelah skrip di atas dijalankan, data dengan nama **tes ekspor data** akan berada satu folder dengan skrip ini. Fungsi `write.csv` digunakan karena data akan kita ekspor ke dalam format .csv. Di sini yang perlu diperhatikan adalah nama data yang akan diekspor, yaitu `data1` dan diikuti dengan nama data dan ekstensinya. 

# Memahami Struktur Dasar Web
Sebuah web yang biasa kita lihat adalah halaman antarmuka (interface) yang disusun menggunakan kode-kode tertentu. Misalnya dalam sebuah artikel, kita bisa menemukan **Huruf Judul Lebih Besar dan Tebal**, sementara isinya menggunakan huruf yang sama. Untuk membedakan tulisan/tampilan atau bagian tulisan/tampilan dari sebuah halaman pembuat web pada umumnya menggunakan kode-kode `html`. Kode-kode tersebutlah yang akan kita manfaatkan sebagai parameter untuk menambang informasi yang tersedia dan terbuka dari sebuah website.

![Gambar 3. Interface Rstudio](/Volumes/mydata/RStudio/belajaR/cdc-workshop/p1.png)
**Keterangan:** Gambar di atas merupakan struktur web jurnal **Policy & Internet** dengan kata kunci pencarian **public policy**

Kode yang didapat untuk judul = `hlFld-Title`

Dalam istilah web (mungkin) baris-baris kode di sebeleh kiri disebut **tag**. Ketika kita ingin melakukan scrapping kita akan lebih banyak menghabiskan waktu untuk mempelajari karakter setiap web yang ingin di ambil datanya umumnya berbeda satu dengan lainnya. Oleh karena itu, kita akan lebih mengandalkan alat lain untuk mendeteksi kode-kode html dalam sebuah web dan diaplikasikan pada kode script untuk scrapping. Alat tersebut disebut `selector gadget`.

# Menggunakan Selector Gadget
Selector gadget dapat digunakan pada browser google chrome dan dapat diunduh/install dengan mengikiti **[tautan ini](https://chrome.google.com/webstore/detail/selectorgadget/mhjhnkcfbdhnjickkkdbjoemdmbfginb?hl=en).** Setelah di install pada browser chorme kita sudah ada simbol selector gedget di bagian kanan-atas. 

Di satu sisi, alat ini sangat mudah digunakan. Namun untuk scrapping yang lebih kompleks, misalnya data dengan multitautan kita tetap harus mengamati kode html secara langsung. 
![Gambar 4. Interface Rstudio](/Volumes/mydata/RStudio/belajaR/cdc-workshop/p2.png)

Di sini kita bisa mendapatkan kode yang sama seperti yang kita amati secara langsung di atas, yaitu kode `hlFld-Title` untuk judul. Dengan kata lain, kemungkinan kita sudah bisa melakukan scrapping. Saya sebut kemungkinan karena terkadang kode antar halam berbeda. Untuk itu scrapping biasanya membutuhkan banyak eksperimen kode, script dan skenario. 

**Catatan:** Lembar ini saya tujukan untuk memberikan intro tentang alat yang akan digunakan untuk melakukan scrapping. Untuk lebih lanjutnya saya akan coba jelaskan melalui percobaan langsung, baik untuk scrapping media sosial dengan API maupun web lain. Jika sudah familiar dengan R atau Rstudio, dan sebelumnya sudah pernah melakukan scrapping web kita bisa langsung ke materi selanjutnya tentang praktik scraping web. 

Di sini saya akan gunakan web jurnal **policy & internet** untuk web scrapping semi otomatis dan **Twitter** untuk scrapping menggunakan API. 